---
title: "Logistic Regression"
author: "Xingche Guo"
date: "4/13/2019"
output: pdf_document
---

# Preprocess
```{r}
library(ggplot2)

Train <- read.csv(file = "/Users/apple/Desktop/ISU 2019 spring/DMC2019/DMC_2019_task/train.csv",
                  sep = "|")

names(Train)

Train$trustLevel <- as.factor(Train$trustLevel)
TotalItem <- Train$totalScanTimeInSeconds * Train$scannedLineItemsPerSecond
AveValue <- Train$grandTotal / TotalItem

Train1 <- data.frame(Train, TotalItem = TotalItem)

level1_ind <- which(as.numeric(Train1$trustLevel)==1)
level2_ind <- which(as.numeric(Train1$trustLevel)==2)
```



# Plot total item vs fraud
```{r}
## all
ggplot(data = Train1) + 
  geom_histogram(aes(x = TotalItem, fill = as.factor(fraud)), binwidth = 2, alpha = 0.8) + 
  facet_grid(~as.factor(fraud)) + 
  theme_bw()

## level1
ggplot(data = Train1[level1_ind,]) + 
  geom_histogram(aes(x = TotalItem, fill = as.factor(fraud)), binwidth = 2, alpha = 0.8) + 
  facet_grid(~as.factor(fraud)) + 
  theme_bw()

## level2
ggplot(data = Train1[level2_ind,]) + 
  geom_histogram(aes(x = TotalItem, fill = as.factor(fraud)), binwidth = 2, alpha = 0.8) + 
  facet_grid(~as.factor(fraud)) + 
  theme_bw()
```



# CV & logistic functions
```{r}
################################## logistic regression ####################################
lossDMC <- function(true, pred){
  loss <- sum( (true==1) & (pred==0) ) * 5 +
    sum( (true==0) & (pred==1) ) * 25 + 
    sum( (true==1) & (pred==1) ) * (-5)
  return(loss)
}





cv_design <- function(n, fold = 10){
  m <- floor(n/fold)
  r <- n%%fold
  p1 <- rep(m, fold)
  p2 <- rep(0, fold)
  if (r>=1){
    p2[1:r] <- 1
  }
  p <- p1 + p2
  ub <- cumsum(p)
  lb <- ub - p + 1
  x <- sample(n)
  IND <- vector("list",fold)
  for (i in 1:fold){
    IND[[i]] <- x[(lb[i]):(ub[i])]
  }
  return(IND)
}




cv_logistic_probs <- function(D,formula, fold = 10){
  n <- length(D[,1])
  IND <- cv_design(n, fold)
  probs <- rep(0,n)
  options(warn=-1) 
  for (i in 1:fold){
    test_ind <- IND[[i]]
    Train <- D[-test_ind,] 
    Test <- D[test_ind,] 
    fit <- glm(formula=formula,data=Train,family=binomial(link=logit))
    py <- predict(fit,newdata=data.frame(Test), type = "response")
    probs[test_ind] <- py
  }
  return(probs)
}
```





# Repeated CV results  (100 repeated, 10 fold, Cross-Validation)

## all trained as 0 (not fraud)
```{r}
pred0 <- rep(0, length(Train[,1]))

## DMC loss
lossDMC(true = Train1$fraud, 
        pred = pred0 )

## 0-1 loss
sum(pred0!=Train1$fraud)
```




## Use total item as covariate
```{r}
formula1 <- "fraud~."
loss1 <- rep(0, 100)
loss2 <- rep(0, 100)
for (i in 1:100){
  pred1 <- pred0 
  probs <- cv_logistic_probs(Train1, formula1, 10)
  pred1[ which(  probs > 5/7  ) ] <- 1
  loss1[i] <- lossDMC(true = Train1$fraud, 
                          pred = pred1)
  loss2[i] <- sum(pred1!=Train1$fraud)
}

## DMC loss
mean(loss1)
## 0-1 loss
mean(loss2)
```





## Not use total item as covariate
```{r}
formula1 <- "fraud~."
loss1 <- rep(0, 100)
loss2 <- rep(0, 100)
for (i in 1:100){
  pred1 <- pred0 
  probs <- cv_logistic_probs(Train, formula1, 10)
  pred1[ which(  probs > 5/7  ) ] <- 1
  loss1[i] <- lossDMC(true = Train$fraud, 
                      pred = pred1)
  loss2[i] <- sum(pred1!=Train$fraud)
}

## DMC loss
mean(loss1)
## 0-1 loss
mean(loss2)
```




# Fit all the data
```{r}
fit_final <- glm(fraud~., data=Train1, family=binomial(link=logit))
summary(fit_final)
pred_final <- pred0
pred_final[which(fit_final$fitted.values>5/7)] <- 1

## DMC loss
lossDMC(true = Train$fraud, 
        pred = pred_final)
## 0-1 loss
sum(pred_final!=Train1$fraud)
```

